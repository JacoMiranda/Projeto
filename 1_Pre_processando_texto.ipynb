{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacoMiranda/Projeto/blob/master/1_Pre_processando_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ff7dc1-b8d5-44df-ae65-c9051beccedd",
      "metadata": {
        "tags": [],
        "id": "77ff7dc1-b8d5-44df-ae65-c9051beccedd"
      },
      "outputs": [],
      "source": [
        "#conda install -c conda-forge spacy\n",
        "#pip install spacy\n",
        "\n",
        "import spacy as sp\n",
        "import sys\n",
        "!{sys.executable} -m spacy download en\n",
        "!{sys.executable} -m spacy download en_core_web_md\n",
        "!{sys.executable} -m spacy download pt_core_news_sm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58025d0-9ffa-4bb4-8ab1-c92d4ace5edc",
      "metadata": {
        "id": "e58025d0-9ffa-4bb4-8ab1-c92d4ace5edc"
      },
      "source": [
        "# Introdução a NLP com SPACY\n",
        "\n",
        "Texto natural, ou texto não-estruturado, sempre foi um grande desafio para sistemas de computação. As gramáticas das linguas naturais são geradas a partir do uso comum, e não o inverso como ocorre nas linguagens de programação.\n",
        "\n",
        "Assim, uma mesma ideia pode ser expressada de diversas formas diferentes, utilizando palavras e estruturas gramaticais diferentes.\n",
        "\n",
        "Isto é um _inferno_ para quem quer extrair informação de texto utilizando computadores. \n",
        "\n",
        "Durante anos, uma subárea da computação tem se focado na leitura, normalização e extração de informações de texto natural: O __Processamento de Linguagem Natural__.\n",
        "\n",
        "Neste curso iremos ver de modo superficial como usar bibliotecas Python chamadas _Spacy_, _gensim_ e _huggingface_ para este processamento. Iniciaremos com a Spacy.\n",
        "\n",
        "Spacy é uma biblioteca extremamente versátil, que pode ser utilizada em diversas linguas. Para tal, você deve baixar um _modelo_ treinado em um _corpus_ específico da linguagem que você pretende processar. \n",
        "\n",
        "Este modelo é um conjunto de técnicas estatísticas e de aprendizagem de máquina que são treinadas usando um conjunto de textos previamente anotados (o tal _corpus_).\n",
        "\n",
        "Exemplo:\n",
        "\n",
        " - _en\\_core\\_web\\_sm_ - Modelo treinado na lingua inglesa com blogs e paginas Web.\n",
        " - _pt_core_news_md_ - Modelo treinado na lingua inglesa com blogs e paginas Web e com vetores pre treinados pra 20k palavras.\n",
        " \n",
        "Como este será um curso eminentemente prático, vamos ver o que isto significa botando a mão na massa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad481af6-9b3c-4b47-83b6-d4ddeb2cbb0a",
      "metadata": {
        "id": "ad481af6-9b3c-4b47-83b6-d4ddeb2cbb0a"
      },
      "outputs": [],
      "source": [
        "nlp = sp.load('en_core_web_sm')\n",
        "pln = sp.load('pt_core_news_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81648c3e-d04a-45f6-9eb2-7328492e2272",
      "metadata": {
        "id": "81648c3e-d04a-45f6-9eb2-7328492e2272"
      },
      "source": [
        "## Processando Linguagem Natural\n",
        "\n",
        "Normalmente, ao trabalhar com texto, o primeiro passo é tentar __estruturar__ as informações. E isto é muito mais complicado do que parece. \n",
        "\n",
        "Uma pessoa genérica, como uma personagem de novela da globo, poderia dizer algo como:\n",
        "\n",
        "> Eu acho que estou com vontade de tomar uma cervejinha. Vamos?\n",
        "\n",
        "Enquanto um Manauara em um contexto informal poderia expressar a mesma idéia na forma:\n",
        "\n",
        "> Rapaz, um cerveja agora hein, eu tomava eu. Bura lá?\n",
        "\n",
        "Apesar da forma, ordem e escolha de palavras radicalmente diferentes, ambas sentenças expressam uma mesma vontade: _tomar uma cerveja_. E o grande desafio é que os programas entendam as duas como __semanticamente equivalentes__.\n",
        "\n",
        "Uma vantagem para vocês é que vivemos no futuro, então temos tudo isso de forma muito mais mastigada e pronta para usar.\n",
        "\n",
        "Em um nível mais alto, chamamos todas as ações para organizar e extrair informações de texto de _pipeline de preprocessamento_:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47a868c7-9589-4a51-8aea-aa238183ec3a",
      "metadata": {
        "id": "47a868c7-9589-4a51-8aea-aa238183ec3a"
      },
      "source": [
        "![image.png](attachment:d931f990-afbf-48c9-beaa-b04625185eb9.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3b1468e-6d4d-40aa-ab28-eb640a407553",
      "metadata": {
        "id": "e3b1468e-6d4d-40aa-ab28-eb640a407553"
      },
      "source": [
        "![image.png](attachment:b5a7084c-2beb-4d22-bfa6-26e4dca53540.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c93bc2e-eee8-434e-a000-993fd3a591fd",
      "metadata": {
        "id": "2c93bc2e-eee8-434e-a000-993fd3a591fd"
      },
      "source": [
        "## Tokenização\n",
        "\n",
        "Normalmente, o primeiro passo ao se processar texto é transformar a _string_, um conjunto(__vetor, na verdade__) de _caracteres_, em um vetor de __palavras__ (e números, pontuação, etc). Este processo é chamado de _tokenização_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a5bad1-5844-4676-932f-e5bb7a8a0cc7",
      "metadata": {
        "id": "e2a5bad1-5844-4676-932f-e5bb7a8a0cc7"
      },
      "outputs": [],
      "source": [
        "dpt = pln(\"Eu acho que estou com vontade de tomar uma cervejinha. Vamos?\")\n",
        "doc = nlp(\"I am flying to NY\")\n",
        "print([w.text for w in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1313058-f2d9-44a8-b122-00b10489f815",
      "metadata": {
        "tags": [],
        "id": "d1313058-f2d9-44a8-b122-00b10489f815"
      },
      "source": [
        "Na função `nlp` a sentença inteira é processada e diversas estruturas são geradas para auxiliar neste processo. \n",
        "\n",
        "Com relação a tokenição, ela gera um objeto com diversas informações a respeito de cada _token_. \n",
        "\n",
        "Para ajudar a visualizar o que o pode ser feito, dêem uma olhada na demo em https://explosion.ai/demos/displacy\n",
        "\n",
        "## Lematização\n",
        "\n",
        "Um lema é a forma básica de um token. Imagine que seria como a forma que um token apareceria em um dicionário, sem plurais e inflexões.\n",
        "\n",
        "Vamos ver como extrair lemas com o Spacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311b4124-9866-4fde-80a9-e0c501c8b2ac",
      "metadata": {
        "id": "311b4124-9866-4fde-80a9-e0c501c8b2ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc87b370-6373-49e5-b5e6-ef1ff23ee8d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this this\n",
            "product product\n",
            "integrates integrate\n",
            "both both\n",
            "libraries library\n",
            "for for\n",
            "downloading download\n",
            "and and\n",
            "applying apply\n",
            "patches patch\n"
          ]
        }
      ],
      "source": [
        "#for token in dpt:\n",
        "for token in nlp(u'this product integrates both libraries for downloading and applying patches'):\n",
        "    print(token.text,token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e402f5a4-0402-4019-98e6-19e551638221",
      "metadata": {
        "id": "e402f5a4-0402-4019-98e6-19e551638221"
      },
      "source": [
        "### Reconhecendo Significados com Lematização\n",
        "\n",
        "Lematização é importante para facilitar o reconhecimento de significados. Veja novamente a frase abaixo:\n",
        "\n",
        "`I am flying to NY`\n",
        "\n",
        "Imagine que a frase foi submetida para um chatbot que gerencia reservas de viagens, podendo estas ser de avião, trem ou ônibus. É importante que o sistema identifique na frase qual ação ele vai tomar (_fly_) e para onde (_New York_).\n",
        "\n",
        "Desta forma, pode-se procurar por verbos específicos de acordo com o modal de transporte, sem se preocupar com inflexões.\n",
        "\n",
        "\n",
        "### Part-of-Speech Tagging (ou análise morfológica)\n",
        "\n",
        "POS tagging é o ato de identificar, para cada palavra, a sua função morfológica (substantivo, verbo, objeto, etc). Isto é importante pois a função de uma palavra pode mudar radicalmente de acordo com o __contexto__ em que ela aparece em uma frase.\n",
        "\n",
        "No spaCy, as POS-tags tem informações detalhadas de cada token, como tempo verbal, pessoa (primeira, segunda, terceira), numeral (singular ou plural), entre outros.\n",
        "\n",
        "Extrair verbos de texto pode ajudar a identificar intenções quando lematização não for o suficiente. Em cenários reais, é muito comum que diversas sentenças \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2791d0b-e73d-494a-9edc-5c48fde7b413",
      "metadata": {
        "id": "e2791d0b-e73d-494a-9edc-5c48fde7b413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fddbdc7c-6706-4d7e-ae2f-a5d33e51c7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John PROPN proper noun\n",
            "and CCONJ coordinating conjunction\n",
            "I PRON pronoun\n",
            "went VERB verb\n",
            "to ADP adposition\n",
            "the DET determiner\n",
            "park NOUN noun\n",
            ". PUNCT punctuation\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(u'John and I went to the park.')\n",
        "for token in doc:\n",
        "    print(token.text,token.pos_,sp.explain(token.pos_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb17d70-5108-4f9c-9111-09fccda7de5b",
      "metadata": {
        "id": "7bb17d70-5108-4f9c-9111-09fccda7de5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6547e82c-c6d5-4d30-d3ed-7190756304d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu PRON pronoun\n",
            "estou VERB verb\n",
            "estudando VERB verb\n",
            "processamento NOUN noun\n",
            "em ADP adposition\n",
            "linguagem NOUN noun\n",
            "natural ADJ adjective\n",
            "com ADP adposition\n",
            "python PROPN proper noun\n"
          ]
        }
      ],
      "source": [
        "#faça um teste aqui com uma frase em portugues\n",
        "frase = pln(u'Eu estou estudando processamento em linguagem natural com python')\n",
        "for token in frase:\n",
        "  print(token.text, token.pos_,sp.explain(token.pos_))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "defb5b14-d920-4ea4-8d97-86f78eef1d59",
      "metadata": {
        "id": "defb5b14-d920-4ea4-8d97-86f78eef1d59"
      },
      "source": [
        "Como você pode ver, o Spacy consegue identificar, para cada palavra, qual a sua função sintática. Como ninguém aqui é obrigado a saber abreviações sintáticas de inglês, tem aqui uma tabelinha pra facilitar suas vidas. _De nada!_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e6fdbba-2fc9-4f62-8aba-04ddc81df05e",
      "metadata": {
        "id": "6e6fdbba-2fc9-4f62-8aba-04ddc81df05e"
      },
      "source": [
        "![image.png](attachment:b88d3cbc-3072-42e0-883d-a57bccb1b600.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d6a81b-b8ca-49f2-8ea9-e25c4244cb35",
      "metadata": {
        "id": "83d6a81b-b8ca-49f2-8ea9-e25c4244cb35"
      },
      "source": [
        "Com análise sintática (ou POS tegging), já é possível extrair muita informação de texto, como quais são os verbos (ações), sujeitos, adjetivos e muito mais.\n",
        "\n",
        "## Exercicio:\n",
        "\n",
        "Faça um trecho de código em python para extrair os verbos dos textos abaixo:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192b7f3f-5ce6-4560-a907-3144f1dc2de3",
      "metadata": {
        "id": "192b7f3f-5ce6-4560-a907-3144f1dc2de3"
      },
      "outputs": [],
      "source": [
        "stri = u'spaCy (/speɪˈsiː/ spay-SEE) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.[3][4] The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage.[5][6] spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, PyTorch or MXNet through its own machine learning library Thinc.[7][8] Using Thinc as its backend, spaCy features convolutional neural network models for part-of-speech tagging, dependency parsing, text categorization and named entity recognition (NER). Prebuilt statistical neural network models to perform these tasks are available for 17 languages, including English, Portuguese, Spanish, Russian and Chinese, and there is also a multi-language NER model. Additional support for tokenization for more than 65 languages allows users to train custom models on their own datasets as well.[9]'\n",
        "case = \"\"\"Navigate to hotmail.com\n",
        "Enter the email address of the registered user in the ’email’ field.\n",
        "Enter the password of the registered user\n",
        " Click the ‘Next’ button.\n",
        "Click ‘Log in’\"\"\"\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fdfefac-2086-4093-a27c-1c5e2550ab4c",
      "metadata": {
        "id": "8fdfefac-2086-4093-a27c-1c5e2550ab4c"
      },
      "source": [
        "## Extraindo dependências sintáticas\n",
        "\n",
        "Muitas vezes, apenas extrair o verbo não é suficiente. Pra poder entender melhor o texto, pode ser importante também a _relação_ entre as palavras. Não basta saber que X é verbo e Y é substantivo, é importante também saber que Y é objeto direto do verbo X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b3e5ac-d207-498d-9b91-d41fd587cd1c",
      "metadata": {
        "id": "43b3e5ac-d207-498d-9b91-d41fd587cd1c"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\")\n",
        "op = {\"compact\": True, \"bg\": \"#09a3d5\",\n",
        "      \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
        "#displacy.render(doc, style=\"dep\",options=op)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bacd35d5-5301-488e-9fea-d0ccb6d16614",
      "metadata": {
        "id": "bacd35d5-5301-488e-9fea-d0ccb6d16614"
      },
      "source": [
        "Como você pode ver bem, a análise sintática gera uma __árvore__ de dependências na sentença (ou múltiplas, a depender). Os rótulos de dependência novamente estão em inglês. Eis alguns comuns:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ff5b64d-5293-4e6d-bbf5-269c724649ab",
      "metadata": {
        "id": "1ff5b64d-5293-4e6d-bbf5-269c724649ab"
      },
      "source": [
        "![image.png](attachment:a46a060c-0bcb-48fd-8e8b-8b3d017aba30.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e611df-9752-4ab9-a4dc-e6097fb719af",
      "metadata": {
        "id": "64e611df-9752-4ab9-a4dc-e6097fb719af"
      },
      "source": [
        "Por ser uma árvore, obviamente ela deve ter um nó raiz (Root) que é um token cuja dependência é de si mesmo. Vamos testar algumas frases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815889f4-92c4-4a68-a39d-fdd8c20d384e",
      "metadata": {
        "id": "815889f4-92c4-4a68-a39d-fdd8c20d384e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c43c9ad-c77b-4bb1-d960-8ae8a5658729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON nsubj nominal subject\n",
            "have AUX aux auxiliary\n",
            "flown VERB ROOT None\n",
            "to ADP prep prepositional modifier\n",
            "LA PROPN pobj object of preposition\n",
            ". PUNCT punct punctuation\n",
            "Now ADV advmod adverbial modifier\n",
            "I PRON nsubj nominal subject\n",
            "am AUX aux auxiliary\n",
            "flying VERB ROOT None\n",
            "to ADP prep prepositional modifier\n",
            "Frisco PROPN pobj object of preposition\n",
            ". PUNCT punct punctuation\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')\n",
        "for token in doc:\n",
        "    print(token.text,token.pos_,token.dep_,sp.explain(token.dep_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bac91cc-c0d2-44a2-bffc-184c2f7973d6",
      "metadata": {
        "id": "9bac91cc-c0d2-44a2-bffc-184c2f7973d6"
      },
      "source": [
        "Legal, mas isso não mostra _de quem_ cada token depende. Para ver isso você pode acessar `token.head` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a996acec-02ad-403e-9ccb-d27b6d7eedd7",
      "metadata": {
        "id": "a996acec-02ad-403e-9ccb-d27b6d7eedd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891e5a27-8e19-4046-e5a7-a0c641193ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON nsubj flown\n",
            "have AUX aux flown\n",
            "flown VERB ROOT flown\n",
            "to ADP prep flown\n",
            "LA PROPN pobj to\n",
            ". PUNCT punct flown\n",
            "Now ADV advmod flying\n",
            "I PRON nsubj flying\n",
            "am AUX aux flying\n",
            "flying VERB ROOT flying\n",
            "to ADP prep flying\n",
            "Frisco PROPN pobj to\n",
            ". PUNCT punct flying\n"
          ]
        }
      ],
      "source": [
        "for token in doc:\n",
        "    print(token.text,token.pos_,token.dep_,token.head.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13605d7-a9e8-4b4a-b310-ce46cf13e489",
      "metadata": {
        "id": "b13605d7-a9e8-4b4a-b310-ce46cf13e489"
      },
      "source": [
        "Um exemplo de como usar isto é, por exemplo, extrair apenas verbos e seus objetos diretos ou indiretos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3421dd63-a176-47d0-9c89-832be6dfd410",
      "metadata": {
        "id": "3421dd63-a176-47d0-9c89-832be6dfd410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db2a0c57-79b4-4c3d-d004-f5808d1be1e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Enter', 'screen', 'Browser']\n"
          ]
        }
      ],
      "source": [
        "for sent in doc.sents: #Extrai sentença por sentença\n",
        "    print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj' or w.dep_ == 'dobj'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0706233-2835-4849-9836-a26fa9c84b7d",
      "metadata": {
        "id": "f0706233-2835-4849-9836-a26fa9c84b7d"
      },
      "source": [
        "### Chunking\n",
        "\n",
        "Como pode ser visto, muitas vezes um conjunto de palavras pode ter um único significado. Por exemplo, na frase abaixo, o atributo noun_chunks guarda os conjuntos de palavras que tem função de locução substantiva:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00c32c4-2cc4-4851-a62d-27d6ba2283e5",
      "metadata": {
        "id": "b00c32c4-2cc4-4851-a62d-27d6ba2283e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b630fce7-a611-4162-bc07-f81f5ba15dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the configuration screen\n",
            "the Web Browser\n"
          ]
        }
      ],
      "source": [
        "sent = \"Enter the configuration screen of the Web Browser.\"\n",
        "doc = nlp(sent)\n",
        "for chunk in doc.noun_chunks:\n",
        "   print(chunk.text)\n",
        "\n",
        "#for chunk in pln(\"Entre na tela de configurações do navegador Web\").noun_chunks:\n",
        " #  print(chunk.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01954430-9561-418a-abfb-ea4b775269f4",
      "metadata": {
        "id": "01954430-9561-418a-abfb-ea4b775269f4"
      },
      "source": [
        "## Navegando nas dependências.\n",
        "\n",
        "De posse da árvore de dependências, podemos navegar pela sentença, tanto usando chunking quanto tokens individuais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46ba9d7-7ae0-4022-9308-e764c7d3b496",
      "metadata": {
        "id": "c46ba9d7-7ae0-4022-9308-e764c7d3b496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7718c496-db5a-4a93-a852-6e3fe6d057cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the configuration screen screen dobj Enter\n",
            "the Web Browser Browser pobj of\n"
          ]
        }
      ],
      "source": [
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "    chunk.root.head.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f6729f-75b1-4bc4-853b-4c4f5d89039a",
      "metadata": {
        "id": "64f6729f-75b1-4bc4-853b-4c4f5d89039a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2969ae8-1b91-4781-9e87-c935f3cf486a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter VERB ROOT Enter VERB [screen, .]\n",
            "the DET det screen NOUN []\n",
            "configuration NOUN compound screen NOUN []\n",
            "screen NOUN dobj Enter VERB [the, configuration, of]\n",
            "of ADP prep screen NOUN [Browser]\n",
            "the DET det Browser PROPN []\n",
            "Web PROPN compound Browser PROPN []\n",
            "Browser PROPN pobj of ADP [the, Web]\n",
            ". PUNCT punct Enter VERB []\n"
          ]
        }
      ],
      "source": [
        "for token in doc:\n",
        "    print(token.text,token.pos_, token.dep_, token.head.text, token.head.pos_,\n",
        "    [child for child in token.children])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5445f39-5688-433d-8f89-bcd8d47115b3",
      "metadata": {
        "id": "b5445f39-5688-433d-8f89-bcd8d47115b3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9acefd71-e061-48f7-9014-214c3059bb6a",
      "metadata": {
        "id": "9acefd71-e061-48f7-9014-214c3059bb6a"
      },
      "source": [
        "### Exercício:\n",
        "\n",
        "Faça funções que:\n",
        "\n",
        "- Retornem todos os verbos de uma string\n",
        "- Retorne pares Verbo, Objeto (direto ou indireto) de uma string\n",
        "- Retorne pares Substantivo, Adjetivos de uma string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def verbos(s,mod=pln):\n",
        "  return [w for w in mod(s) if w.pos_ == 'VERB']\n",
        "\n",
        "def verbosObj(s):\n",
        "  return 'vobj'\n",
        "\n",
        "def subsAdj(s):\n",
        "  return 'sadj'\n",
        "verbos(\"testando e programando para utilizar multiplos modelos\",)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETFTM23NUlDC",
        "outputId": "78021d29-b61b-4b6a-ed76-dd7a57fc156f"
      },
      "id": "ETFTM23NUlDC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[testando, programando, utilizar]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e250f03-ccb8-4b93-a72f-63de47bfd8ba",
      "metadata": {
        "id": "7e250f03-ccb8-4b93-a72f-63de47bfd8ba"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9310e5d-5085-43f3-9cad-7d74f6347cb6",
      "metadata": {
        "id": "f9310e5d-5085-43f3-9cad-7d74f6347cb6"
      },
      "source": [
        "## Reconhecendo entidades nomeadas\n",
        "\n",
        "Uma entidade nomeada é qualquer objeto que pode ser chamado por um nome próprio (ou seja, um substantivo próprio). Uma atividade comum em processamento de texto é a extração de entidades, bem como a identificação de qual tipo de entidade é esta.\n",
        "\n",
        "O processamento padrão do Spacy já identifica entidades nomeadas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c1cf844-4706-4ada-bcef-d958d6fd660a",
      "metadata": {
        "id": "6c1cf844-4706-4ada-bcef-d958d6fd660a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c509a5-07e7-482a-ba48-f78b2ff4415a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LA GPE\n",
            "Frisco ORG\n"
          ]
        }
      ],
      "source": [
        "\n",
        "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')\n",
        "for token in doc:\n",
        "    if token.ent_type != 0:\n",
        "        print(token.text, token.ent_type_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a37ddb-4b00-45c6-90ab-01d28aa93cc6",
      "metadata": {
        "id": "80a37ddb-4b00-45c6-90ab-01d28aa93cc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ed5522-291f-42a7-afd4-b68c6e2692a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Microsoft 0 9 ORG\n",
            "Europe 31 37 LOC\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(u'Microsoft has offices all over Europe.')\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d493848-5ce6-4574-a466-c0803f27a044",
      "metadata": {
        "id": "3d493848-5ce6-4574-a466-c0803f27a044"
      },
      "source": [
        "Entidades nomeadas podem ter os seguintes tipos:\n",
        "\n",
        " - PERSON: Pessoas, incluindo fictícias.\n",
        " - NORP: Nacionalidades, religiões e grupos politicos.\n",
        " - FACILITY: Predios, aeroportos, pontes, etc.\n",
        " - ORG: Empresas, instituições, etc.\n",
        " - GEP: Cidade, estado ou país.\n",
        " - LOC: Localidades não GEP (montanhas, rios, etc).\n",
        " - PRODUCT: Objetos, veículos, comida e etc.\n",
        " - EVENT: Batalhas, eventos esportivos, furacões, guerras, etc.\n",
        " - WORK_OF_ART: Títulos de Livros, músicas, filmes, etc.\n",
        " - LAW: Documentos legais nomeados.\n",
        " - LANGUAGE: Nome de linguas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ec183de-92a3-45c1-8f36-da764fabc862",
      "metadata": {
        "id": "3ec183de-92a3-45c1-8f36-da764fabc862"
      },
      "outputs": [],
      "source": [
        "#Verifique aqui o funcionamento em português.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a512a9-d38d-4736-9032-15eb5ca04025",
      "metadata": {
        "id": "e0a512a9-d38d-4736-9032-15eb5ca04025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5a85e2-c963-4e12-96aa-d48148321bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "George Washington 0 17 PERSON\n",
            "American 25 33 NORP\n",
            "first 119 124 ORDINAL\n",
            "the United States 138 155 GPE\n",
            "1789 to 1797 161 173 DATE\n"
          ]
        }
      ],
      "source": [
        "ex = \"George Washington was an American political leader, \\\n",
        "military general, statesman, and Founding Father who served as the \\\n",
        "first president of the United States from 1789 to 1797.\\n\"\n",
        "\n",
        "doc = nlp(ex)\n",
        "for token in doc.ents:\n",
        "    print(token.text, token.start_char, token.end_char, token.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ca0b02b-2afb-4922-a065-2aca3438f742",
      "metadata": {
        "id": "1ca0b02b-2afb-4922-a065-2aca3438f742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4e7f760f-1918-462f-fcef-8ac962ca9fcf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    George Washington\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was an \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " political leader, military general, statesman, and Founding Father who served as the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " president of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the United States\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " from \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1789 to 1797\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "displacy.render(doc, style='ent', jupyter=True, options={'distance': 120})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd25734-dc7a-46e0-b49c-89064001868a",
      "metadata": {
        "id": "6cd25734-dc7a-46e0-b49c-89064001868a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "name": "1-Pre-processando texto.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}